ğŸŒ¦ï¸ Mizoram Weather & Flooding Data ETL Pipeline
A mini-production grade ETL pipeline using PySpark, designed to process weather and flood data for various places in Mizoram, India. This project simulates monthly data ingestion from multiple source tables, performs aggregations, runs data quality checks, and loads summarized insights into a target table for reporting and analytics.

ğŸ“Œ Project Objective
âœ… Process monthly weather and flood readings from source tables.
âœ… Perform data quality checks on incoming data.
âœ… Calculate aggregated metrics (average temperature, total rainfall, etc.).
âœ… Join aggregated data with master data for place details.
âœ… Load final summarized data into a target table for each unique place ID along with month date and load status.
âœ… Implement logging and configuration-driven design for production-readiness.

ğŸ“Š Tech Stack
ğŸ Python 3.x

âš¡ PySpark (Spark SQL, DataFrames)

ğŸ˜ PostgreSQL (via pgAdmin for source/target tables)

ğŸ“¦ Config-driven pipeline (JSON config)

ğŸ“‘ Logging framework (Custom Python logger)

ğŸ“‚ Project Structure

etl_mini_project/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ etl_config.json             # Pipeline config file (table names, month date, load status etc.)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample CSVs (if any)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl_job.py                  # Main ETL job script
â”‚   â”œâ”€â”€ db_config.py                # Spark session & table read/write utilities
â”‚   â”œâ”€â”€ data_quality.py             # Data quality check functions
â”‚   â””â”€â”€ logger.py                   # Logging setup
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ etl_job.log                 # ETL run logs
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # Project documentation

ğŸ“– How the Pipeline Works
Configuration Load:
Reads pipeline parameters from etl_config.json.

Source Table Load:
Loads place_master, weather_readings, and flood_alerts tables from PostgreSQL using Spark JDBC.

Data Quality Checks:

Null checks for primary columns.

Minimum row count validation.

Uniqueness check for primary keys.

Data Transformation:

Filters records up to the configured month_date.

Aggregates:

average temperature

total rainfall

average humidity

max water level

flood risk level

Joins aggregated stats with place_master.

Final Data Load:

Writes summary records into the mizoram_weather_summary target table via JDBC.

Logging & Monitoring:
Logs every major operation for easy debugging and production tracking.

ğŸ“ˆ Example Aggregations
Metric	Source Table	Calculation
Avg Temperature	weather_readings	AVG(temperature)
Total Rainfall	weather_readings	SUM(rainfall)
Avg Humidity	weather_readings	AVG(humidity)
Max Water Level	flood_alerts	MAX(water_level)
Max Flood Risk Level	flood_alerts	MAX(flood_risk_level)

ğŸ“ How to Run
Update PostgreSQL connection details and table names in config/etl_config.json.

Install dependencies:

pip install -r requirements.txt
Run the ETL Job:

python src/etl_job.py
ğŸ“‘ Sample Config â€” etl_config.json

{
  "log_path": "logs/etl_job.log",
  "month_date": "2025-06-01",
  "load_status_id": 1,
  "tables": {
    "place_master": "place_master",
    "weather_readings": "weather_readings",
    "flood_alerts": "flood_alerts",
    "target_summary": "mizoram_weather_summary"
  }
}
ğŸ“Œ Future Enhancements
Automate monthly data load scheduling via Airflow / AWS Glue

Add schema validation (column type checks)

Integrate Delta Lake / Hudi for incremental loads

Build dashboards in Tableau / Metabase for insights

ğŸ“œ License
This project is for learning, demonstration, and interview preparation purposes. No production use without adaptation and testing.
